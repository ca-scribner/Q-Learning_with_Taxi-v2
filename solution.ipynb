{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABYCAYAAAC5+driAAAEx0lEQVR4Ae2cv28TMRTHvwcIBAgJhJBatR3KwAQIUVU0ZavEwEgyEaYuSIgNsaUwhJT/AJCQEFPTqSBYGJhJkCoY+DUAokhplEqIiQUGZOQ0zv1I7vJ8Z/cu4UU6nc9+fn7++NmxnZwdIYQAfyIJ7IpM5cQ2AYZEcASGlATS1moBhdUtgophFqlj2VlGfUAV2JMGAGony28376dVzctvO/9VrnlFRiBcE5VgHZEXK43+VUP/aCEkrHy15UuulV14bpq3QE9BjRWR7xpSEQqzrxE88Hdet6yatN21zVdZz4MWJE++EQnSIDmytpRu+T/L8MBNaH2GxJAIBAgioZ5kczJpUzehzh4Rnkx6YCQLhnpSj9rWJ3xcfIV3nevj8x89IpmNeL0Mx3E6VwHVTT1L6ZCk3tkJnH58HqfvHgGeNtFSZW1WUXD0C1fZI++auut3HDh3PKsxCSgH1ISQc0IIsYbiZGSJPYl7emKoEeN7sZ8qm6Jc/eUSKnWBXAIb9CCtN/FusQngII49PoPDquDJItZEUT2ZvWvqzt0S7bXTthFb2Hifx/QiAOmRU1fwBGhDK83RzYzR3U7g0Pgf/O72NXphOy85hulTT7Ahx6A2bIFaWd8KPUht/ccwXdiHX0+/u6VpjhtuRkJIU3dwTMpdqGApN3jPKMqSGJAAzEzg0GYTX95Eqc5I2lwJrepbzHe+3eZvx7ArbDnfb6skTFY33qZuPVtouwDxPClGYwxzFt4qIbReqCdlZ31FqEVsEV67xUYXzBjqSUHB//nZDKSEc5nIBrCpO7JgN9EMJFffSIb01m5hCBKtr8KUduJt6h5QtEpmT1IkIu5mINkcN2zqjgDjTTIDyatxBMM8JhEalT2JAInXbgRIoZ7EazeXXigkV4RDDIngA2Yg2ZzL2NRNACRFzEAiFjasYjxPIrQce9KOQbI5btjUTQAkRdiTCKB4TCJAYk8iQLKydvv24mTfoo9f/NA3PuuRoZ6kt3aj/X4VD0b6ukMhxavQaOZiSIR2TQVS8D9EBDvJIjZ0pwKJXOOMCJqZJ2lWxv+/Rs3MA8Rt6GZPGgBdJqcCyca4oepqQ3cqkFSFhuXOYxKhpdiTCJB47UaAZOhFZdpfffX+Pqyk09fN3Y3gSURIciXuf1VL7hL4XpkiFGZGpIHlq+twutdbVC2fFkKElEOpUcDajSq27anjUfEsarfivSBlYi5TuTYL8XAWtZm/uPKs0eVvQndXWSdAnwJMFnHvUgHXVxdw8+s8kPAdsqAh8Z5/YqMJ5M8diJedmIsOCcDY5Zs464xjvlyD0HhfLGiLifXV0oN1LLUV78bKzNFuESZ0d5V1AsTuprLlsFAGKhfidTOlxcTd193uf+4MAyY09+rQhNSrIE6MjXFD2WFDt1Z3U4Zk4e7rbuUTGLNolDYk2eeTdrZk48YUSg+nUAqBkkx3f6WpdLf+pmQ31sraLbvVjWcZexKBG0NiSAQCBJFQT9L7mZtQUiZFaD+hh0LKZJ3SMkptbam774hDdQSi56hDJTfcd7mR5x4HuX2upuf4x0DltHYmd/4sSG9lPJUwcoalJEHb9dSCFAA8Ao80SDyZJIxzPHAzJAIBggh7EkMiECCIsCcxJAIBgsg/OceaTY6aTocAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Below is a solution for the OpenAI Gym [Taxi-v2](https://gym.openai.com/envs/Taxi-v2/) environment using simple Q-learning.  \n",
    "\n",
    "Taxi is a game whereby the player must transport a passenger from one location to another.  The board is fixed with some obstacles, but the pickup and dropoff locations are varied between plays between four standard locations.  A example board is:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "where possible pickup and dropoff locations are denoted by letters, and the specific pickup and dropoff locations for this instance of the game are denoted by blue and purple text, respectively.  \n",
    "\n",
    "Valid actions in the game are movements north/south/east/west and attempting a pickup or dropoff.  The reward structure for moves is:\n",
    "\n",
    "* Successful dropoff: +20 points\n",
    "* Illeagle pickup/dropoff attempt: -10 points\n",
    "* All other moves: -1 point\n",
    "\n",
    "Within the world, any vertical bar represents a wall that cannot be traversed.  Moving into a wall leaves the agent where they started before the move.  \n",
    "\n",
    "The below solution was crafted to try basic Q-learning with different search strategies ($\\epsilon$-greedy and full exploration), and see how that affects the speed at which an agent learns the optimal solution and also the full Q function (including non-optimal states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from utilities import Agent, run_episode, run_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, a dict of move translations for each human understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of valid move translation (for printing/manual use), just in case we want it\n",
    "moves = {\n",
    "    's': 0,\n",
    "    'n': 1,\n",
    "    'e': 2,\n",
    "    'w': 3,\n",
    "    'p': 4,\n",
    "    'd': 5,\n",
    "}\n",
    "# Inverse moves\n",
    "moves.update({v: k for k, v in moves.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the taxi environment (do not initialize here - we initialize during looping of run_episodes below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our agent settings.  We two search strategies:\n",
    "\n",
    "* $\\epsilon$-Greedy: The agent will choose an action based on:\n",
    "$$P=\\begin{cases}\n",
    "1-\\epsilon + \\frac{\\epsilon}{N_{actions}}, choose best action\\\\\n",
    "\\frac{\\epsilon}{N_{actions}}, all other actions\\\\\n",
    "\\end{cases}$$\n",
    "    Thus we choose the best action 1-$\\epsilon$ of the time, choosing a random action the remaining time.  This has the effect of typically marching through the current best guess at an optimal policy, but exploring with probability of $\\epsilon$\n",
    "\n",
    "* Exploration: The agent will always take the action it has previously taken the least.  This prioritizes exploration and establishing a good estimate of the Q function over exploitation of the environment (although whenever we evaluate the current \"best\" policy of this agent, we of course use a greedy policy that takes the best action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'e-greedy (0.1)': {'strategy':'egreedy', 'epsilon': 0.1},\n",
    "    'e-greedy (0.3)': {'strategy':'egreedy', 'epsilon': 0.3},\n",
    "    'e-greedy (0.5)': {'strategy':'egreedy', 'epsilon': 0.5},\n",
    "    'Exploration-First': {'strategy':'exploration', },\n",
    "}\n",
    "\n",
    "report = {}\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate each learner by letting the agents play the game 4000 times.  For every 100 plays, report:\n",
    "* r_recent: Average total reward obtained playing the game from the most recent 100 plays of the evironment (the reward we get using our search strategy)\n",
    "* r_greedy: Average total reward obtained playing the game using the current best strategy (greedy strategy) learned by the agent, applied to 100 additional random plays of the environment.  This number represents how well the learner is actually learning the optimal gameplay strategy and is a proxy for convergence on the optimal policy\n",
    "* dq_max_recent: Maximum change in the Q-function estimate being learned by the agent.  This is a rough proxy for convergence on learning the entire game (not just the optimal policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running case e-greedy (0.1)\n",
      "0: r_recent = -713.00, r_greedy = -831.49, dq_max_recent = 0.0000\n",
      "100: r_recent = -299.80, r_greedy = -236.00, dq_max_recent = 11.3907\n",
      "200: r_recent = -210.05, r_greedy = -218.00, dq_max_recent = 10.7717\n",
      "300: r_recent = -154.69, r_greedy = -193.66, dq_max_recent = 6.8617\n",
      "400: r_recent = -106.71, r_greedy = -265.12, dq_max_recent = 5.5394\n",
      "500: r_recent = -85.28, r_greedy = -216.79, dq_max_recent = 4.4455\n",
      "600: r_recent = -57.17, r_greedy = -184.17, dq_max_recent = 4.7428\n",
      "700: r_recent = -33.95, r_greedy = -212.77, dq_max_recent = 2.8413\n",
      "800: r_recent = -25.49, r_greedy = -167.37, dq_max_recent = 2.8151\n",
      "900: r_recent = -22.47, r_greedy = -184.90, dq_max_recent = 2.7278\n",
      "1000: r_recent = -12.09, r_greedy = -125.51, dq_max_recent = 3.2249\n",
      "1100: r_recent = -7.49, r_greedy = -88.50, dq_max_recent = 2.2059\n",
      "1200: r_recent = -1.59, r_greedy = -72.11, dq_max_recent = 2.5469\n",
      "1300: r_recent = -1.73, r_greedy = -88.08, dq_max_recent = 2.0009\n",
      "1400: r_recent = -3.24, r_greedy = -95.18, dq_max_recent = 1.4317\n",
      "1500: r_recent = -0.02, r_greedy = -59.81, dq_max_recent = 2.1134\n",
      "1600: r_recent = 0.97, r_greedy = -65.32, dq_max_recent = 1.9963\n",
      "1700: r_recent = -0.38, r_greedy = -47.15, dq_max_recent = 2.9395\n",
      "1800: r_recent = 1.24, r_greedy = -48.99, dq_max_recent = 1.8075\n",
      "1900: r_recent = 0.36, r_greedy = -19.93, dq_max_recent = 1.8191\n",
      "2000: r_recent = 1.54, r_greedy = -14.42, dq_max_recent = 1.4641\n",
      "2100: r_recent = 2.39, r_greedy = -24.32, dq_max_recent = 2.2723\n",
      "2200: r_recent = 2.10, r_greedy = -18.65, dq_max_recent = 1.2674\n",
      "2300: r_recent = 2.53, r_greedy = -9.83, dq_max_recent = 0.9934\n",
      "2400: r_recent = 1.79, r_greedy = 0.16, dq_max_recent = 2.1073\n",
      "2500: r_recent = 2.79, r_greedy = -14.07, dq_max_recent = 1.6449\n",
      "2600: r_recent = 1.79, r_greedy = 2.44, dq_max_recent = 2.2035\n",
      "2700: r_recent = 2.41, r_greedy = -5.91, dq_max_recent = 0.8742\n",
      "2800: r_recent = 2.48, r_greedy = -0.35, dq_max_recent = 1.6915\n",
      "2900: r_recent = 2.83, r_greedy = 4.10, dq_max_recent = 1.5216\n",
      "3000: r_recent = 2.30, r_greedy = -1.75, dq_max_recent = 1.0927\n",
      "3100: r_recent = 2.68, r_greedy = -2.00, dq_max_recent = 1.7261\n",
      "3200: r_recent = 2.74, r_greedy = -3.64, dq_max_recent = 1.7739\n",
      "3300: r_recent = 2.25, r_greedy = 8.24, dq_max_recent = 1.2331\n",
      "3400: r_recent = 2.32, r_greedy = 8.91, dq_max_recent = 1.7709\n",
      "3500: r_recent = 3.01, r_greedy = 8.67, dq_max_recent = 1.0464\n",
      "3600: r_recent = 4.42, r_greedy = 8.73, dq_max_recent = 1.3289\n",
      "3700: r_recent = 4.36, r_greedy = 6.12, dq_max_recent = 1.5189\n",
      "3800: r_recent = 2.73, r_greedy = 4.27, dq_max_recent = 1.2235\n",
      "3900: r_recent = 3.62, r_greedy = 6.67, dq_max_recent = 1.1935\n",
      "Running case e-greedy (0.3)\n",
      "0: r_recent = -695.00, r_greedy = -788.91, dq_max_recent = 0.0000\n",
      "100: r_recent = -386.70, r_greedy = -236.00, dq_max_recent = 12.2516\n",
      "200: r_recent = -278.40, r_greedy = -218.00, dq_max_recent = 9.2400\n",
      "300: r_recent = -228.95, r_greedy = -193.60, dq_max_recent = 9.7542\n",
      "400: r_recent = -191.03, r_greedy = -284.65, dq_max_recent = 6.8981\n",
      "500: r_recent = -134.80, r_greedy = -170.16, dq_max_recent = 5.3520\n",
      "600: r_recent = -105.04, r_greedy = -153.39, dq_max_recent = 4.0228\n",
      "700: r_recent = -74.59, r_greedy = -145.24, dq_max_recent = 5.3492\n",
      "800: r_recent = -53.89, r_greedy = -115.80, dq_max_recent = 3.5583\n",
      "900: r_recent = -41.52, r_greedy = -107.38, dq_max_recent = 2.0680\n",
      "1000: r_recent = -34.38, r_greedy = -92.94, dq_max_recent = 3.1353\n",
      "1100: r_recent = -22.52, r_greedy = -78.23, dq_max_recent = 2.2813\n",
      "1200: r_recent = -22.49, r_greedy = -103.39, dq_max_recent = 1.7095\n",
      "1300: r_recent = -20.25, r_greedy = -88.78, dq_max_recent = 1.8399\n",
      "1400: r_recent = -18.44, r_greedy = -61.25, dq_max_recent = 1.6986\n",
      "1500: r_recent = -20.71, r_greedy = -66.15, dq_max_recent = 2.0177\n",
      "1600: r_recent = -15.54, r_greedy = -42.96, dq_max_recent = 2.3319\n",
      "1700: r_recent = -13.44, r_greedy = -45.18, dq_max_recent = 1.6771\n",
      "1800: r_recent = -15.08, r_greedy = -24.19, dq_max_recent = 1.9736\n",
      "1900: r_recent = -13.54, r_greedy = -22.50, dq_max_recent = 1.4473\n",
      "2000: r_recent = -18.10, r_greedy = -32.46, dq_max_recent = 1.6184\n",
      "2100: r_recent = -13.83, r_greedy = -22.36, dq_max_recent = 1.7866\n",
      "2200: r_recent = -12.69, r_greedy = -27.08, dq_max_recent = 1.2789\n",
      "2300: r_recent = -10.41, r_greedy = -1.61, dq_max_recent = 2.2566\n",
      "2400: r_recent = -13.40, r_greedy = -4.08, dq_max_recent = 1.8575\n",
      "2500: r_recent = -14.07, r_greedy = -6.03, dq_max_recent = 1.3045\n",
      "2600: r_recent = -15.40, r_greedy = -4.02, dq_max_recent = 1.8844\n",
      "2700: r_recent = -12.86, r_greedy = 6.67, dq_max_recent = 1.5118\n",
      "2800: r_recent = -14.47, r_greedy = 2.50, dq_max_recent = 1.3471\n",
      "2900: r_recent = -12.14, r_greedy = -1.86, dq_max_recent = 1.2864\n",
      "3000: r_recent = -14.53, r_greedy = 2.10, dq_max_recent = 1.8732\n",
      "3100: r_recent = -14.08, r_greedy = 6.28, dq_max_recent = 1.3541\n",
      "3200: r_recent = -12.27, r_greedy = 8.44, dq_max_recent = 1.6372\n",
      "3300: r_recent = -16.58, r_greedy = -3.83, dq_max_recent = 1.1143\n",
      "3400: r_recent = -16.37, r_greedy = 6.21, dq_max_recent = 1.5483\n",
      "3500: r_recent = -15.13, r_greedy = 4.24, dq_max_recent = 1.3031\n",
      "3600: r_recent = -15.08, r_greedy = -0.16, dq_max_recent = 0.9014\n"
     ]
    }
   ],
   "source": [
    "for k in settings:\n",
    "    print(f\"Running case {k}\")\n",
    "    agent = Agent(n_states, n_actions, alpha=0.1, gamma=0.9, **settings[k])\n",
    "    report[k] = run_episodes(env, agent, imax=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some simple visualization of the results:\n",
    "\n",
    "* Plot recent and greedy rewards (described above) versus plays of the game\n",
    "* Plot dq versus plays of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 10))\n",
    "for k in report:\n",
    "    axs[0].plot(report[k]['i'], report[k]['r_greedy'], label=k + ' (greedy)')\n",
    "    c = axs[0].get_lines()[-1].get_color()\n",
    "#     ax.plot(report[k]['i'], np.array(report[k]['r_greedy'])-500, label=k)\n",
    "    axs[0].plot(report[k]['i'], report[k]['r_window'], color=c, ls='--', label=k + ' (recent)')\n",
    "    axs[1].plot(report[k]['i'], report[k]['dq_max_window'], color=c, label=k)\n",
    "\n",
    "axs[0].set(xlabel=\"Number of Games Played\", \n",
    "           ylabel=\"Mean Average Total Reward\")\n",
    "axs[1].set(xlabel=\"Number of Games Played\", \n",
    "           ylabel=\"Max Change in Q Estimate\")\n",
    "    \n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[0].set_ylim(-200, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize how consistent the results are for the greedy policies.  Below shows the greedy policies from the above figure overlaid with a shaded area showing +-1 standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "for k in report:\n",
    "    ax.plot(report[k]['i'], report[k]['r_greedy'])\n",
    "    c = ax.get_lines()[-1].get_color()\n",
    "#     ax.plot(report[k]['i'], np.array(report[k]['r_greedy'])-500, label=k)\n",
    "    ax.fill_between(report[k]['i'], \n",
    "                    np.array(report[k]['r_greedy']) - np.array(report[k]['r_greedy_std']),\n",
    "                    np.array(report[k]['r_greedy']) + np.array(report[k]['r_greedy_std']), color=c, alpha=0.5,\n",
    "                             label=k + ' (greedy) +-1 std dev')\n",
    "\n",
    "ax.set(xlabel=\"Number of Games Played\", \n",
    "           ylabel=\"Mean Average Total Reward\")\n",
    "ax.legend()\n",
    "ax.set_ylim(-200, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see the exploration strategy in this case is the most efficient to finding a good policy.  This is likely influenced by how simple this environment is (only ~500 states, 6 actions per state) - in cases with more complex state spaces we'd expect much worse performance from an exploration-first strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aigym]",
   "language": "python",
   "name": "conda-env-aigym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
